{
 "metadata": {
  "name": "",
  "signature": "sha256:fa56e6a09a08301a717187a2470cffc36d5170944fa33a32aab22028d92dde7b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "HW0: MultiClass SVM"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "This exercise guides you through the process of classifying images using Support Vector Machines(SVM). As part of this you will:\n",
      "-Implement a fully vectorized loss function and for the SVM\n",
      "-Calculate the analytical gradient using vectorized code\n",
      "-Use Validation to tune the hyper-parameters\n",
      "-perform Stochastic Gradient Descent (SGD) to minimize the loss function\n",
      "-Visualize the learned weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# start-up code to import data-sets\n",
      "from f15ece6504.get_cifar10 import load_CIFAR10\n",
      "# loading some necessary packages\n",
      "import numpy as np\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# This is to make matplotlib figures appear inline in the\n",
      "# notebook rather than in a new window.\n",
      "%matplotlib inline\n",
      "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
      "plt.rcParams['image.interpolation'] = 'nearest'\n",
      "plt.rcParams['image.cmap'] = 'gray'\n",
      "\n",
      "# Some code so that the notebook will reload external python modules;\n",
      "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Loading and Pre-processing CIFAR-10"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Assuming you have downloaded the CIFAR-10 database in your HW0-f15ece6504/f15ece6504/data folder, \n",
      "# we proceed to load the data into python\n",
      "cifar10_dir = 'f15ece6504/data/cifar-10-batches-py'\n",
      "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
      "\n",
      "# As a sanity check, we print out the size of the training and test data.\n",
      "print 'Training data shape: ', X_train.shape\n",
      "print 'Training labels shape: ', y_train.shape\n",
      "print 'Test data shape: ', X_test.shape\n",
      "print 'Test labels shape: ', y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Visualize some examples from the dataset.\n",
      "# We show a few examples of training images from each class.\n",
      "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "num_classes = len(classes)\n",
      "samples_per_class = 7\n",
      "for y, cls in enumerate(classes):\n",
      "    idxs = np.flatnonzero(y_train == y)\n",
      "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
      "    for i, idx in enumerate(idxs):\n",
      "        plt_idx = i * num_classes + y + 1\n",
      "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
      "        plt.imshow(X_train[idx].astype('uint8'))\n",
      "        plt.axis('off')\n",
      "        if i == 0:\n",
      "            plt.title(cls)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Subsample the data for more efficient code execution in this exercise.\n",
      "num_training = 49000\n",
      "num_validation = 1000\n",
      "num_test = 1000\n",
      "\n",
      "# Our validation set will be num_validation points from the original\n",
      "# training set.\n",
      "mask = range(num_training, num_training + num_validation)\n",
      "X_val = X_train[mask]\n",
      "y_val = y_train[mask]\n",
      "\n",
      "# Our training set will be the first num_train points from the original\n",
      "# training set.\n",
      "mask = range(num_training)\n",
      "X_train = X_train[mask]\n",
      "y_train = y_train[mask]\n",
      "\n",
      "# We use the first num_test points of the original test set as our\n",
      "# test set.\n",
      "mask = range(num_test)\n",
      "X_test = X_test[mask]\n",
      "y_test = y_test[mask]\n",
      "\n",
      "print 'Train data shape: ', X_train.shape\n",
      "print 'Train labels shape: ', y_train.shape\n",
      "print 'Validation data shape: ', X_val.shape\n",
      "print 'Validation labels shape: ', y_val.shape\n",
      "print 'Test data shape: ', X_test.shape\n",
      "print 'Test labels shape: ', y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Preprocessing: reshape the image data into rows\n",
      "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
      "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
      "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
      "\n",
      "# As a sanity check, print out the shapes of the data\n",
      "print 'Training data shape: ', X_train.shape\n",
      "print 'Validation data shape: ', X_val.shape\n",
      "print 'Test data shape: ', X_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Preprocessing: subtract the mean image\n",
      "# first: compute the image mean based on the training data\n",
      "mean_image = np.mean(X_train, axis=0)\n",
      "print mean_image[:10] # print a few of the elements\n",
      "plt.figure(figsize=(4,4))\n",
      "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# second: subtract the mean image from train and test data\n",
      "X_train -= mean_image\n",
      "X_val -= mean_image\n",
      "X_test -= mean_image"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
      "# only has to worry about optimizing a single weight matrix W.\n",
      "# Also, lets transform both data matrices so that each image is a column.\n",
      "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]).T\n",
      "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]).T\n",
      "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]).T\n",
      "# print X_train[0][-1]\n",
      "print X_train.shape, X_val.shape, X_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "SVM Classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Code for this section will all be written inside f15ece6504/classifiers/linear_svm.py.\n",
      "\n",
      "As you can see, we have prefilled the function compute_loss_naive which uses for loops to evaluate the multiclass SVM loss function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "\n",
      "# generate a random SVM weight matrix of small numbers\n",
      "W = np.random.randn(10,3073) * 0.0001 \n",
      "\n",
      "# Implement a vectorized code to implement the loss and the gradient of the SVM. \n",
      "# Remember that your code should not have any loops. \n",
      "# Hint: \n",
      "# Look up the broadcasting property of np arrays\n",
      "# In case you have memory errors, you might want to use float16 or pickle your data\n",
      "# check np.ndarray.dump()/load() if you consider pickling your data\n",
      "                    \n",
      "tic = time.time()\n",
      "_, grad_vectorized = svm_loss_vectorized(W, X_train, y_train, 0.00001)\n",
      "toc = time.time()\n",
      "print 'Vectorized loss and gradient: computed in %fs' % (toc - tic)\n",
      "\n",
      "# It should be quite fast and finish within a second if your code is optimized! "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stochastic Gradient Descent (SGD)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now that we have efficient implementations for computing loss and gradients, let us use SGD to minimize \n",
      "# loss funtion"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now implement SGD in LinearSVM.train() function and run it with the code below\n",
      "from f15ece6504.classifiers import LinearSVM\n",
      "svm = LinearSVM()\n",
      "tic = time.time()\n",
      "loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=5e4,\n",
      "                      num_iters=1500, verbose=True)\n",
      "toc = time.time()\n",
      "print 'That took %fs' % (toc - tic)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A useful debugging strategy is to plot the training loss as a function of\n",
      "# iteration number:\n",
      "plt.plot(loss_hist)\n",
      "plt.xlabel('Iteration number')\n",
      "plt.ylabel('Loss value')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write the LinearSVM.predict function and evaluate the performance on both the\n",
      "# training and validation set\n",
      "y_train_pred = svm.predict(X_train)\n",
      "print 'training accuracy: %f' % (np.mean(y_train == y_train_pred), )\n",
      "y_val_pred = svm.predict(X_val)\n",
      "print 'validation accuracy: %f' % (np.mean(y_val == y_val_pred), )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tuning the parameters\n",
      "In our classifier, the learning rate and the regularization strength are hyper-parameters. Tuning them to appropriate values is neede to bring out the best from these classifiers. Generally, this takes experience and is generally considered a dark art! Fortunately, it is not necessary for you to become the dark lord! Spearmint is a software package which does this using Bayesian Optimization.\n",
      "\n",
      "Assuming you have installed spearmint as mentioned in the README, we proceed to its set-up and usage. Spearmint needs you to create two files:\n",
      "1. A wrapper file that calls the classifier, trains it and returns the error on the validation set\n",
      "2. A config file that has details about the parameters to be tuned, language, etc. \n",
      "In you Spearmint installation directory, you have many examples that should give you an idea. \n",
      "\n",
      "For SVM, you are provided with these two files in the hw0/ directory. Complete them appropriately and proceed to tuning your model. \n",
      "\n",
      "```sh\n",
      "$ mongod --fork --logpath <path/to/logfile\\> --dbpath <path/to/dbfolder\\>\n",
      "$ python /path/to/spearmint_install/spearmint/main.py </path/to/hw0\\>\n",
      "```\n",
      "\n",
      "Once you tune the model, use these parameters to retrain the classifier (modify and run the first cell in the SGD section and proceed to the next). Spearmint saves the output of each iteration in hw0/output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Visualize the learned weights for each class.\n",
      "# Depending on your choice of learning rate and regularization strength, these may\n",
      "# or may not be nice to look at.\n",
      "w = svm.W[:,:-1] # strip out the bias\n",
      "w = w.reshape(10, 32, 32, 3)\n",
      "w_min, w_max = np.min(w), np.max(w)\n",
      "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "for i in xrange(10):\n",
      "  plt.subplot(2, 5, i + 1)\n",
      "    \n",
      "  # Rescale the weights to be between 0 and 255\n",
      "  wimg = 255.0 * (w[i].squeeze() - w_min) / (w_max - w_min)\n",
      "  plt.imshow(wimg.astype('uint8'))\n",
      "  plt.axis('off')\n",
      "  plt.title(classes[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}